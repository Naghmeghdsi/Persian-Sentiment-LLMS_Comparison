# Persian-Sentiment-LLMS_Comparison
Code and experiments for a course project on adapting large language models to Persian sentiment analysis. We compare a prompt‑based baseline with a parameter‑efficient fine‑tuning (PEFT) approach on the ParsiAI/digikala-sentiment-analysis dataset, and analyse the trade‑offs between accuracy, training cost and number of trainable parameters.
